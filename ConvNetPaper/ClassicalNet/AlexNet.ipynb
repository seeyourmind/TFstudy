{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding=utf-8 -*-\n",
    "'''\n",
    "AlexNet：\n",
    "GPU0: in-[卷积+ReLu-池化+标准化]-[卷积-ReLu-池化-标准化]+[卷积-RuLu]+[卷积-RuLu]+[卷积-RuLu]+池化-[全连接+dropout]-[全连接+dropout]-out0\n",
    "GPU1: in-[卷积+ReLu-池化]-[卷积+ReLu-池化]+[卷积-RuLu]+[卷积-RuLu]+[卷积-RuLu]+池化-[全连接+dropout]-[全连接+dropout]-out1\n",
    "out: out0.join.out1\n",
    "---------------------\n",
    "input: 227×227×3\n",
    "output: 1000\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# 配置神经网络参数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# 配置图像数据参数\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "# 第一层卷积层\n",
    "CONV1_SIZE = 11\n",
    "CONV1_DEEP = 48\n",
    "CONV1_STRIDES = 4\n",
    "POOL1_KSIZE = 3\n",
    "POOL1_STRIDES = 2\n",
    "# 第一层卷积层\n",
    "CONV2_SIZE = 5\n",
    "CONV2_DEEP = 128\n",
    "CONV2_STRIDES = 1\n",
    "POOL2_KSIZE = 3\n",
    "POOL2_STRIDES = 2\n",
    "# 第一层卷积层\n",
    "CONV3_SIZE = 3\n",
    "CONV3_DEEP = 192\n",
    "CONV3_STRIDES = 1\n",
    "# 第一层卷积层\n",
    "CONV4_SIZE = 3\n",
    "CONV4_DEEP = 192\n",
    "CONV4_STRIDES = 1\n",
    "# 第一层卷积层\n",
    "CONV5_SIZE = 3\n",
    "CONV5_DEEP = 128\n",
    "CONV5_STRIDES = 1\n",
    "POOL5_KSIZE = 3\n",
    "POOL5_STRIDES = 2\n",
    "# 全连接层的节点个数\n",
    "FC_SIZE = 2048\n",
    "OUT_SIZE = 1000\n",
    "\n",
    "'''\n",
    "获取本地GPU个数\n",
    "'''\n",
    "def get_local_gpu_num():\n",
    "    gn = 0\n",
    "    for x in device_lib.list_local_devices():\n",
    "        if x.device_type == 'GPU':\n",
    "            gn += 1\n",
    "    return gn\n",
    "\n",
    "'''\n",
    "生成weights变量[get_varibale]\n",
    "支持正则化损失函数\n",
    "'''\n",
    "def get_weight_variable(shape, stddev=0.1, regularizer=None):\n",
    "    weights = tf.get_variable('weights', shape, initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses', regularizer(weights))  \n",
    "    return weights\n",
    "\n",
    "'''\n",
    "生成biases变量[get_varibale]\n",
    "'''\n",
    "def get_biase_variable(shape, initv=0.0):\n",
    "    biases = tf.get_variable('biases', shape, initializer=tf.constant_initializer(initv))\n",
    "    return biases\n",
    "\n",
    "# 定义卷积+ReLu操作\n",
    "def conv_relu_op(input_tensor, size, channel, deep, strides, padding='SAME'):\n",
    "    weights = get_weight_variable([size, size, channel, deep])\n",
    "    biases = get_biase_variable([deep])\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, strides=[1,strides,strides,1], padding=padding)\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "    return relu\n",
    "\n",
    "# 定义最大池化操作\n",
    "def max_pool_op(input_tensor, ksize=3, strides=1, padding='SAME'):\n",
    "    return tf.nn.max_pool(input_tensor, ksize=[1, ksize, ksize, 1], strides=[1, strides, strides, 1], padding=padding)\n",
    "\n",
    "# 定义标准化操作\n",
    "def norm_op(input_tensor, lsize=4):\n",
    "    return tf.nn.lrn(input_tensor, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "'''\n",
    "定义卷积神经网络前向传播过程\n",
    "支持滑动平均模型\n",
    "支持正则化损失函数\n",
    "添加train，用于区分训练过程和测试过程\n",
    "'''\n",
    "def inference(input_tensor, train, avg_class, regularizer, reuse=Fasle):\n",
    "    if get_local_gpu_num() >= 2:\n",
    "        with tf.device('/gpu_0'):\n",
    "            fc_gpu0 = inference_gpu0(input_tensor, train, avg_class, regularizer)\n",
    "        with tf.device('/gpu_1'):\n",
    "            fc_gpu1 = inference_gpu1(input_tensor, train, avg_class, regularizer)\n",
    "    else:\n",
    "        fc_gpu0 = inference_gpu0(input_tensor, train, avg_class, regularizer)\n",
    "        fc_gpu1 = inference_gpu1(input_tensor, train, avg_class, regularizer)\n",
    "    \n",
    "    fc = tf.concat(2, [fc_gpu0, fc_gpu1])\n",
    "    # 定义最后一层输出层\n",
    "    with tf.variable_scope('layer8-softmax', reuse=reuse):\n",
    "        # 只有全连接层的权重需要加入正则化\n",
    "        weights = get_weight_variable([FC_SIZE*2, OUT_SIZE], regularizer=regularizer)\n",
    "        biases = get_biase_variable([OUT_SIZE], initv=0.1)\n",
    "        # 使用relu激活函数\n",
    "        logit = tf.matmul(fc, weights)+biases\n",
    "        \n",
    "    return logit\n",
    "    \n",
    "    \n",
    "def inference_gpu0(input_tensor, train, avg_class, regularizer, reuse=False):\n",
    "    # 定义第一层卷积层：输入28×28×1 输出28×28×32\n",
    "    with tf.variable_scope('layer1-conv-gpu0', reuse=reuse):\n",
    "        conv1 = conv_relu_op(input_tensor, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP, CONV1_STRIDES)\n",
    "        pool1 = max_pool_op(conv1, POOL1_KSIZE, POOL1_STRIDES)\n",
    "        \n",
    "    # 定义第二层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer2-conv-gpu0', reuse=reuse):\n",
    "        conv2 = conv_relu_op(pool1, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP, CONV2_STRIDES)\n",
    "        pool2 = max_pool_op(conv2, POOL2_KSIZE, POOL2_STRIDES)\n",
    "        \n",
    "    # 定义第三层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer3-conv-gpu0', reuse=reuse):\n",
    "        conv3 = conv_relu_op(pool2, CONV3_SIZE, CONV3_DEEP, CONV3_DEEP, CONV3_STRIDES)\n",
    "    \n",
    "    # 定义第四层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer4-conv-gpu0', reuse=reuse):\n",
    "        conv4 = conv_relu_op(conv3, CONV4_SIZE, CONV3_DEEP, CONV4_DEEP, CONV4_STRIDES)\n",
    "    \n",
    "    # 定义第五层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer5-conv-gpu0', reuse=reuse):\n",
    "        conv5 = conv_relu_op(conv4, CONV5_SIZE, CONV4_DEEP, CONV5_DEEP, CONV5_STRIDES)\n",
    "        pool5 = max_pool_op(conv5, POOL5_KSIZE, POOL5_STRIDES)\n",
    "        \n",
    "    # 将卷积层结果转为FC层输入格式\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool5, [pool_shape[0], nodes])\n",
    "    \n",
    "    # 定义第六层全连接层：输入3136×1 输出512×1\n",
    "    with tf.variable_scope('layer6-fc-gpu0', reuse=reuse):\n",
    "        # 只有全连接层的权重需要加入正则化\n",
    "        weights = get_weight_variable([nodes, FC_SIZE], regularizer=regularizer)\n",
    "        biases = get_biase_variable([FC_SIZE], initv=0.1)\n",
    "        # 使用relu激活函数\n",
    "        fc6 = tf.nn.relu(tf.matmul(reshaped, weights)+biases)\n",
    "        # dropout避免过拟合：在训练过程中会随机将部分节点输出为0\n",
    "        if train: fc1 = tf.nn.dropout(fc6, 0.5)\n",
    "    \n",
    "    # 定义第七层全连接层：输入3136×1 输出512×1\n",
    "    with tf.variable_scope('layer7-fc-gpu0', reuse=reuse):\n",
    "        # 只有全连接层的权重需要加入正则化\n",
    "        weights = get_weight_variable([FC_SIZE, FC_SIZE], regularizer=regularizer)\n",
    "        biases = get_biase_variable([FC_SIZE], initv=0.1)\n",
    "        # 使用relu激活函数\n",
    "        fc7 = tf.nn.relu(tf.matmul(fc6, weights)+biases)\n",
    "        # dropout避免过拟合：在训练过程中会随机将部分节点输出为0\n",
    "        if train: fc7 = tf.nn.dropout(fc7, 0.5)\n",
    "        \n",
    "    return fc7\n",
    "\n",
    "def inference_gpu1(input_tensor, train, avg_class, regularizer, reuse=False):\n",
    "    # 定义第一层卷积层：输入28×28×1 输出28×28×32\n",
    "    with tf.variable_scope('layer1-conv-gpu1', reuse=reuse):\n",
    "        conv1 = conv_relu_op(input_tensor, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP, CONV1_STRIDES)\n",
    "        pool1 = max_pool_op(conv1, POOL1_KSIZE, POOL1_STRIDES)\n",
    "        norm1 = norm_op(pool1)\n",
    "        \n",
    "    # 定义第二层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer2-conv-gpu1', reuse=reuse):\n",
    "        conv2 = conv_relu_op(norm1, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP, CONV2_STRIDES)\n",
    "        pool2 = max_pool_op(conv2, POOL2_KSIZE, POOL2_STRIDES)\n",
    "        norm2 = norm_op(pool2)\n",
    "        \n",
    "    # 定义第三层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer3-conv-gpu1', reuse=reuse):\n",
    "        conv3 = conv_relu_op(norm2, CONV3_SIZE, CONV3_DEEP, CONV3_DEEP, CONV3_STRIDES)\n",
    "    \n",
    "    # 定义第四层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer4-conv-gpu1', reuse=reuse):\n",
    "        conv4 = conv_relu_op(conv3, CONV4_SIZE, CONV3_DEEP, CONV4_DEEP, CONV4_STRIDES)\n",
    "    \n",
    "    # 定义第五层卷积层：输入28×28×32 输出14×14×32\n",
    "    with tf.variable_scope('layer5-conv-gpu1', reuse=reuse):\n",
    "        conv5 = conv_relu_op(conv4, CONV5_SIZE, CONV4_DEEP, CONV5_DEEP, CONV5_STRIDES)\n",
    "        pool5 = max_pool_op(conv5, POOL5_KSIZE, POOL5_STRIDES)\n",
    "        \n",
    "    # 将卷积层结果转为FC层输入格式\n",
    "    pool_shape = pool5.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool5, [pool_shape[0], nodes])\n",
    "    \n",
    "    # 定义第六层全连接层：输入3136×1 输出512×1\n",
    "    with tf.variable_scope('layer6-fc-gpu1', reuse=reuse):\n",
    "        # 只有全连接层的权重需要加入正则化\n",
    "        weights = get_weight_variable([nodes, FC_SIZE], regularizer=regularizer)\n",
    "        biases = get_biase_variable([FC_SIZE], initv=0.1)\n",
    "        # 使用relu激活函数\n",
    "        fc6 = tf.nn.relu(tf.matmul(reshaped, weights)+biases)\n",
    "        # dropout避免过拟合：在训练过程中会随机将部分节点输出为0\n",
    "        if train: fc1 = tf.nn.dropout(fc6, 0.5)\n",
    "    \n",
    "    # 定义第七层全连接层：输入3136×1 输出512×1\n",
    "    with tf.variable_scope('layer7-fc-gpu1', reuse=reuse):\n",
    "        # 只有全连接层的权重需要加入正则化\n",
    "        weights = get_weight_variable([FC_SIZE, FC_SIZE], regularizer=regularizer)\n",
    "        biases = get_biase_variable([FC_SIZE], initv=0.1)\n",
    "        # 使用relu激活函数\n",
    "        fc7 = tf.nn.relu(tf.matmul(fc6, weights)+biases)\n",
    "        # dropout避免过拟合：在训练过程中会随机将部分节点输出为0\n",
    "        if train: fc7 = tf.nn.dropout(fc7, 0.5)\n",
    "        \n",
    "    return fc7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
